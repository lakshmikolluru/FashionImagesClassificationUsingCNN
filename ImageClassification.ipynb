{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clothes Image Classifier","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\n# link to Data: https://www.kaggle.com/competitions/fiu-cap5610-spring22/data","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:55:08.402898Z","iopub.execute_input":"2023-06-18T23:55:08.403475Z","iopub.status.idle":"2023-06-18T23:55:08.439147Z","shell.execute_reply.started":"2023-06-18T23:55:08.403334Z","shell.execute_reply":"2023-06-18T23:55:08.438166Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Reading the Dataset","metadata":{}},{"cell_type":"code","source":"X_Y_train_img = pd.read_csv(\"/kaggle/input/project/train.csv\")\nX_test_img  = pd.read_csv(\"/kaggle/input/project/test.csv\")\nprint(X_Y_train_img.shape, X_test_img.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:55:08.441186Z","iopub.execute_input":"2023-06-18T23:55:08.441474Z","iopub.status.idle":"2023-06-18T23:55:08.496488Z","shell.execute_reply.started":"2023-06-18T23:55:08.441440Z","shell.execute_reply":"2023-06-18T23:55:08.495484Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(30684, 2) (13151, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Loading all the images whose image ID is present in the training set.<br>\nEach image is adjusted to 32 x 32 pixels and is converted into an array.<br>\nX_train consists of all the training images (array).**","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing import image\nimport matplotlib.pyplot as plt\nIMG_HT = 32\nIMG_WD = 32\n\nimg_path = '/kaggle/input/images/images/'\ntrain_image = []\ntrain_label = []\nfor i in range(0, len(X_Y_train_img)):\n    img = image.load_img(img_path + X_Y_train_img['img_id'][i].astype('str')+'.jpg', target_size=(IMG_HT, IMG_WD), grayscale=False)\n    img = image.img_to_array(img)\n    img = img/255\n    train_image.append(img)\n\nX_train = np.array(train_image)\nY_train = X_Y_train_img['label'].values\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:55:08.498316Z","iopub.execute_input":"2023-06-18T23:55:08.498874Z","iopub.status.idle":"2023-06-18T23:57:45.441997Z","shell.execute_reply.started":"2023-06-18T23:55:08.498825Z","shell.execute_reply":"2023-06-18T23:57:45.439521Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(30684, 32, 32, 3)"},"metadata":{}}]},{"cell_type":"code","source":"Y_train = pd.get_dummies(Y_train)\nY_train = Y_train.to_numpy()\nY_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:57:45.443447Z","iopub.execute_input":"2023-06-18T23:57:45.443718Z","iopub.status.idle":"2023-06-18T23:57:45.465537Z","shell.execute_reply.started":"2023-06-18T23:57:45.443680Z","shell.execute_reply":"2023-06-18T23:57:45.464546Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(30684, 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"****Loading all the images whose image ID is present in the test set.<br>\nEach image is adjusted to 32 x 32 pixels and is converted into an array.<br>\nX_test consists of all the test images (array).****","metadata":{}},{"cell_type":"code","source":"X_Y_test_img  = pd.read_csv(\"/kaggle/input/project/sample_submission.csv\")\nimg_path = '/kaggle/input/images/images/'\ntest_image = []\ntest_label = []\nfor i in range(0, len(X_Y_test_img)):\n    img = image.load_img(img_path + X_Y_test_img['img_id'][i].astype('str')+'.jpg', target_size=(IMG_HT, IMG_WD), grayscale=False)\n    img = image.img_to_array(img)\n    img = img/255\n    test_image.append(img)\n\nX_test = np.array(test_image)\nY_test = X_Y_test_img['label'].values\nX_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:57:45.469442Z","iopub.execute_input":"2023-06-18T23:57:45.469746Z","iopub.status.idle":"2023-06-18T23:58:48.737910Z","shell.execute_reply.started":"2023-06-18T23:57:45.469711Z","shell.execute_reply":"2023-06-18T23:58:48.736879Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(13151, 32, 32, 3)"},"metadata":{}}]},{"cell_type":"code","source":"Y_test = pd.get_dummies(Y_test)\nY_test.head()\nY_test = Y_test.to_numpy()\nY_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:58:48.739532Z","iopub.execute_input":"2023-06-18T23:58:48.740266Z","iopub.status.idle":"2023-06-18T23:58:48.752501Z","shell.execute_reply.started":"2023-06-18T23:58:48.740213Z","shell.execute_reply":"2023-06-18T23:58:48.751389Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(13151, 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Building the model using Convolutional Networks\nThe model consists of 2 convolution layers with 32, 32, 48, 48 number of nodes in each layer respectively and 2 fully connected layers with 256 and 64 nodes respectively.<br>\nFinal output layer consists of 4 nodes with activation softmax.<br>\nAll other layers (convolution and fully-connected) use relu activation.<br>\nAverage Pooling are also used.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import Input\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import BatchNormalization\n\nmodel = Sequential()\n\n#LeNet modified 2 Accuracy = 0.98342\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(IMG_HT, IMG_WD, 3)))\nmodel.add(AveragePooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))\nmodel.add(AveragePooling2D(pool_size=2, strides=2))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(84, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:58:48.753937Z","iopub.execute_input":"2023-06-18T23:58:48.754313Z","iopub.status.idle":"2023-06-18T23:58:49.309605Z","shell.execute_reply.started":"2023-06-18T23:58:48.754267Z","shell.execute_reply":"2023-06-18T23:58:49.308689Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### The model runs for 20 epochs. Adam optimizier is used along with a batch size of 128 examples is used in fitting the model.","metadata":{}},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:58:49.311275Z","iopub.execute_input":"2023-06-18T23:58:49.311648Z","iopub.status.idle":"2023-06-18T23:58:49.333436Z","shell.execute_reply.started":"2023-06-18T23:58:49.311601Z","shell.execute_reply":"2023-06-18T23:58:49.332485Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=20, batch_size=128)\n# model.fit(X_train, Y_train, epochs=20, batch_size=128, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-06-18T23:58:49.334966Z","iopub.execute_input":"2023-06-18T23:58:49.335346Z","iopub.status.idle":"2023-06-19T00:04:53.453295Z","shell.execute_reply.started":"2023-06-18T23:58:49.335302Z","shell.execute_reply":"2023-06-19T00:04:53.452035Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/20\n240/240 [==============================] - 19s 77ms/step - loss: 0.3648 - accuracy: 0.8681\nEpoch 2/20\n240/240 [==============================] - 18s 73ms/step - loss: 0.1774 - accuracy: 0.9396\nEpoch 3/20\n240/240 [==============================] - 18s 77ms/step - loss: 0.1286 - accuracy: 0.9585\nEpoch 4/20\n240/240 [==============================] - 18s 73ms/step - loss: 0.1035 - accuracy: 0.9678\nEpoch 5/20\n240/240 [==============================] - 19s 78ms/step - loss: 0.0872 - accuracy: 0.9723\nEpoch 6/20\n240/240 [==============================] - 19s 78ms/step - loss: 0.0744 - accuracy: 0.9766\nEpoch 7/20\n240/240 [==============================] - 18s 74ms/step - loss: 0.0629 - accuracy: 0.9803\nEpoch 8/20\n240/240 [==============================] - 19s 77ms/step - loss: 0.0522 - accuracy: 0.9837\nEpoch 9/20\n240/240 [==============================] - 18s 73ms/step - loss: 0.0456 - accuracy: 0.9854\nEpoch 10/20\n240/240 [==============================] - 18s 77ms/step - loss: 0.0397 - accuracy: 0.9872\nEpoch 11/20\n240/240 [==============================] - 18s 74ms/step - loss: 0.0358 - accuracy: 0.9890\nEpoch 12/20\n240/240 [==============================] - 18s 77ms/step - loss: 0.0314 - accuracy: 0.9899\nEpoch 13/20\n240/240 [==============================] - 18s 76ms/step - loss: 0.0281 - accuracy: 0.9914\nEpoch 14/20\n240/240 [==============================] - 18s 73ms/step - loss: 0.0239 - accuracy: 0.9926\nEpoch 15/20\n240/240 [==============================] - 18s 76ms/step - loss: 0.0254 - accuracy: 0.9924\nEpoch 16/20\n240/240 [==============================] - 18s 74ms/step - loss: 0.0202 - accuracy: 0.9935\nEpoch 17/20\n240/240 [==============================] - 18s 77ms/step - loss: 0.0201 - accuracy: 0.9936\nEpoch 18/20\n240/240 [==============================] - 18s 74ms/step - loss: 0.0164 - accuracy: 0.9947\nEpoch 19/20\n240/240 [==============================] - 18s 77ms/step - loss: 0.0135 - accuracy: 0.9960\nEpoch 20/20\n240/240 [==============================] - 18s 77ms/step - loss: 0.0145 - accuracy: 0.9956\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7d7c855449d0>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Making predictions on the test set","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(X_test)\nY_Pred = pd.DataFrame(prediction, columns = ['Accessories', 'Apparel', 'Footwear', 'Personal Care'])\nY_Pred.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T00:04:53.455112Z","iopub.execute_input":"2023-06-19T00:04:53.455385Z","iopub.status.idle":"2023-06-19T00:04:56.755048Z","shell.execute_reply.started":"2023-06-19T00:04:53.455351Z","shell.execute_reply":"2023-06-19T00:04:56.754328Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"    Accessories   Apparel      Footwear  Personal Care\n0  9.580058e-01  0.033982  2.188428e-04   7.793465e-03\n1  1.065984e-10  1.000000  4.357935e-11   9.797486e-18\n2  6.777984e-10  1.000000  1.711469e-10   4.444432e-25\n3  1.543593e-06  0.999998  6.203912e-08   4.591465e-14\n4  2.567884e-08  1.000000  9.172440e-08   2.323130e-13","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accessories</th>\n      <th>Apparel</th>\n      <th>Footwear</th>\n      <th>Personal Care</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9.580058e-01</td>\n      <td>0.033982</td>\n      <td>2.188428e-04</td>\n      <td>7.793465e-03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.065984e-10</td>\n      <td>1.000000</td>\n      <td>4.357935e-11</td>\n      <td>9.797486e-18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6.777984e-10</td>\n      <td>1.000000</td>\n      <td>1.711469e-10</td>\n      <td>4.444432e-25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.543593e-06</td>\n      <td>0.999998</td>\n      <td>6.203912e-08</td>\n      <td>4.591465e-14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.567884e-08</td>\n      <td>1.000000</td>\n      <td>9.172440e-08</td>\n      <td>2.323130e-13</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Writing the predictions off to a .csv file","metadata":{}},{"cell_type":"code","source":"output_df = pd.DataFrame()\noutput_df['img_id'] = X_Y_test_img['img_id']\noutput_df['label'] = Y_Pred.idxmax(axis=1)\noutput_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T00:04:56.756418Z","iopub.execute_input":"2023-06-19T00:04:56.757234Z","iopub.status.idle":"2023-06-19T00:04:56.803014Z","shell.execute_reply.started":"2023-06-19T00:04:56.757191Z","shell.execute_reply":"2023-06-19T00:04:56.801968Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   img_id        label\n0   26726  Accessories\n1   26241      Apparel\n2   41082      Apparel\n3    2838      Apparel\n4   23533      Apparel","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26726</td>\n      <td>Accessories</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26241</td>\n      <td>Apparel</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41082</td>\n      <td>Apparel</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2838</td>\n      <td>Apparel</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23533</td>\n      <td>Apparel</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"output_df.to_csv('output.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T00:04:56.804463Z","iopub.execute_input":"2023-06-19T00:04:56.804727Z","iopub.status.idle":"2023-06-19T00:04:56.845994Z","shell.execute_reply.started":"2023-06-19T00:04:56.804696Z","shell.execute_reply":"2023-06-19T00:04:56.844993Z"},"trusted":true},"execution_count":12,"outputs":[]}]}